
<!-- 
NEED TO INSERT THE BODY AND ALL THE USEFUL STUFF HERE
-->

<html>

<!-- Mirrored from www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 01 Aug 2021 16:45:01 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<!-- title should be set at this point in page.php -->
<title>Rasterization: a Practical Implementation (An Overview of the Rasterization Algorithm)</title>

<meta name='author' content="Scratchapixel">
<meta name='copyright' content="&copy; 2009-2016 Scratchapixel">
<meta name='keywords' content='rasterizer, rasteriser, rasterisation, rasterization, visibility, frame-buffer, image-buffer, depth-buffer, z-buffer, pixel, raster element, perspective projection, NDC space, inside-outside test, coverage test, edge function, barycentric coordinates, counter-clockwise ordering, winding, face culling, vertex attribute, rasterization rules, top-left rule, hidden surface removal, interpolation, perspective correct interpolation, anti-aliasing'>
<meta name='date' content='2015-01-25 08:05:22'>

<link rel="stylesheet" type="text/css" href="../../css/scratchapixel.css"/>
<link rel="stylesheet" type="text/css" href="../../css/page.css"/>

<link href='../../../fonts.googleapis.com/css16e1.css?family=Noto+Sans' rel='stylesheet' type='text/css'/>
<link href='../../../fonts.googleapis.com/css9908.css?family=Open+Sans:700,400,300' rel='stylesheet' type='text/css'/>

<!-- using the material icons from Google http://google.github.io/material-design-icons/ -->
<link rel="stylesheet" href="../../../fonts.googleapis.com/icone91f.css?family=Material+Icons">

<script type="text/x-mathjax-config">
MathJax.Hub.Config(
	{tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, 
	displayAlign: "left",
	displayIndent: "1em",
	"HTML-CSS": { scale: 90 },
	jax: ["input/TeX","input/MathML","input/AsciiMath","output/HTML-CSS","output/NativeMML"],
	extensions: ["tex2jax.js","../../indexf17c.html","../../indexf17c.html","../../indexf17c.html","MathZoom.js"],
	TeX: {
		extensions: ["AMSmath.js","../../indexf17c.html","../../indexf17c.html","noUndefined.js"]
	}}
);
</script>

<script type="text/javascript" src="../../../cdn.mathjax.org/mathjax/latest/MathJaxdda6.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/javascript"
	src="../../scratchapixelSDK.js">
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','../../../www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-42771397-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- 
https://developers.facebook.com/docs/plugins/share-button/
Include the JavaScript SDK on your page once, ideally right after the opening <body> tag.
-->

<script>
  window.fbAsyncInit = function() {
	FB.init({
	  appId      : '1535346446701691',
	  xfbml      : true,
	  version    : 'v2.1'
	});
  };

  (function(d, s, id){
	 var js, fjs = d.getElementsByTagName(s)[0];
	 if (d.getElementById(id)) {return;}
	 js = d.createElement(s); js.id = id;
	 js.src = "../../../connect.facebook.net/en_US/sdk.js";
	 fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>

<script>
	!function(d,s,id) {
		var js,fjs=d.getElementsByTagName(s)[0];
		if(!d.getElementById(id)) {
			js=d.createElement(s);js.id=id;
			js.src="../../../platform.twitter.com/widgets.js";
			fjs.parentNode.insertBefore(js,fjs);
		}
	}(document,"script","twitter-wjs");
</script>


	<!-- Open Graph data -->
	<meta property='fb:app_id' content='1535346446701691'/>
	<meta property='og:title' content='Rasterization: a Practical Implementation'/>
	<meta property='og:type' content='article'/>
	<meta property='og:url' content='https://www.scratchapixel.com//lessons/3d-basic-rendering/rasterization-practical-implementation'/>
	<meta property='og:image' content='https://www.scratchapixel.com/images/design/logo_scratchapixel2015.png'/>
	<meta property='og:description' content='Lesson summary: This lesson aim at providing you with the mathematical tools that you need in order to generate an image using the rasterization algorithm.'/> 
	<meta property='og:site_name' content='Scratchapixel'/>
	<meta property='fb:admins' content='1260067275'/>
	
</head>

<style>
.full-page1
{
	position: relative;
	margin: 0; padding: 0; 
	width: 100%; 
	border: 1px solid red;
	color: white;
}

.full-page-header-table1
{
	display: table; 
	width: 100%;
	border: 1px solid orange;
}
</style>

<div style='display: table; width: 100%;'>
	<!-- 
		This should be the content of the top menu bar. So if you want to reuse
		this, just create a table div above and insert the section.
		The height here doesn't mater because it's controlled by the inner table.
-->
<div style='display: table-row;  height: 40px; width: 100%; background-color: #1565C0; color: white;'>
	<!-- we center the cell in the middle of the row -->
	<div style='display: table-cell; width: 100%; border: 1px none orange; vertical-align: middle; text-align: center;'>
		
		<div style='display: inline-block; position: relative; border: 1px none black; width: 670px; margin: 0 auto; padding: 0;'>			
			<!-- 
				This is our logo 
			-->
			<div style='display: inline-block; border: 2px none green;'>
				<span style='font-size: 18px; font-weight: 800;'><a href='../../indexf17c.html' style='text-decoration: none; color: white;'>Scratchapixel 2.0</a></span>
			</div>
			<!-- 
				This is login 
				The only way you can vertically center stuff here is by forcing the height of the div
				and the eight of the text to be the same
			-->
			<div style='border: 2px none green; position: absolute; right: 0;  top: 0; bottom: 0px; height: 20px; line-height: 20px; margin: auto 0;'>
				<a style="color: white; text-decoration: none;" href="https://www.facebook.com/v2.5/dialog/oauth?client_id=1682406642074489&amp;state=6241aa28239e903f6cad376e5bc19dd1&amp;response_type=code&amp;sdk=php-sdk-5.4.0&amp;redirect_uri=https%3A%2F%2Fwww.scratchapixel.com%2Ffb-callback.php%3F&amp;scope=email">Sign in</a>			</div>
			
		</div>
	</div>
</div></div>

<body onload="onload();">

<div id='fb-root'></div> <!-- that's required by FB -->
<div class="page-content">
<article><div id='sap-root'></div><!-- that's needed by scratchapixel see onload() --><div class='lesson-title'>Rasterization: a Practical Implementation</div><div style='width: 100%; margin: 0px; display: table; vertical-align: middle; border: 1px none red;'>
			<div style='display: table-row; height: 30px;'>
			<div style='margin: 0px; padding: 0px; border: 1px none blue ; display: table-cell; vertical-align: middle;'>
				<a href='https://twitter.com/share' class='twitter-share-button' data-lang='en'>Tweet</a>
			</div></div>
			<div style='display: table-row; height: 30px;'>
			<div style='padding: 0px; border: 1px none blue ; display: table-cell; vertical-align: middle;'>
				<div style='margin-top: 5px;' class='fb-like' data-share='true' data-width='450' data-show-faces='true'></div>
			</div></div></div><div class='chapter-table'><div class='chapter-row'><div class='chapter-cell'><b>Contents</b></div></div><div class='chapter-row'><div class='chapter-cell'><a style='text-decoration: none;' href="rasterization-practical-implementation/overview-rasterization-algorithm.html"><b>An Overview of the Rasterization Algorithm</b></a></div></div><div class='chapter-row'><div class='chapter-cell'><a style='text-decoration: none;' href="rasterization-practical-implementation/projection-stage.html">The Projection Stage</a></div></div><div class='chapter-row'><div class='chapter-cell'><a style='text-decoration: none;' href="rasterization-practical-implementation/rasterization-stage.html">The Rasterization Stage</a></div></div><div class='chapter-row'><div class='chapter-cell'><a style='text-decoration: none;' href="rasterization-practical-implementation/visibility-problem-depth-buffer-depth-interpolation.html">The Visibility Problem, the Depth Buffer Algorithm and Depth Interpolation</a></div></div><div class='chapter-row'><div class='chapter-cell'><a style='text-decoration: none;' href="rasterization-practical-implementation/perspective-correct-interpolation-vertex-attributes.html">Perspective Correct Interpolation and Vertex Attributes</a></div></div><div class='chapter-row'><div class='chapter-cell'><a style='text-decoration: none;' href="rasterization-practical-implementation/rasterization-practical-implementation.html">Rasterization: a Practical Implementation</a></div></div><div class='chapter-row'><div class='chapter-cell'><a style='text-decoration: none;' href="../../code3559.html?id=26&amp;origin=/lessons/3d-basic-rendering/rasterization-practical-implementation">Source Code</a></div></div></div><span style='display: inline-block; width: 100%; margin: 0px 0px 10px 0px;'><i>Keywords: rasterizer, rasteriser, rasterisation, rasterization, visibility, frame-buffer, image-buffer, depth-buffer, z-buffer, pixel, raster element, perspective projection, NDC space, inside-outside test, coverage test, edge function, barycentric coordinates, counter-clockwise ordering, winding, face culling, vertex attribute, rasterization rules, top-left rule, hidden surface removal, interpolation, perspective correct interpolation, anti-aliasing.</i></span><h2>Everything You Wanted to Know About the Rasterization Algorithm (But Were Afraid to Ask!)</h2>

<p>The <b>rasterization rendering technique</b> is surely the most commonly used technique to render images of 3D scenes, and yet, that is probably the least understood and the least <b>properly</b> documented technique of all (especially compared to ray-tracing).<p>

<p>Why this is so, depends on different factors. First, it's a technique from the past. We don't mean to say the technique is obsolete, quite the contrary, but that most of the techniques that are used to produce an image with this algorithm, were developed somewhere between the 1960s and the early 1980s. In the world of computer graphics, this is middle-ages and the knowledge about the papers in which these techniques were developed tends to be lost. Rasterization is also the technique used by GPUs to produce 3D graphics. Hardware technology changed a lot since GPUs were first invented, but the fondamental techniques they implement to produce images haven't changed much since the early 1980s (the hardware changed, but the underlying pipeline by which an image is formed hasn't). In fact these techniques are so fondamental and consequently so deeply integrated within the hardware architecture, that no one pays attention to them anymore (only people designing GPUs can tell what they really do, and this is far from being a trivial task; but designing a GPU and understanding the principle of the rasterization algorithm are two different things; thus explaining the latter should actually not be that hard!).</p>

<p>Regardless, we thought it was urgent and important to correct this situation. With this lesson, we believe to be the first ressource that provides a clear and complete picture of the algorithm as well as a complete and full practical implementation of the technique. If you found in this lesson the answers you have been desperately looking for anywhere else, please consider making a donation! This work is provided to you for free and requires many hours of hard work.</p>

<h2>Introduction</h2>

<p>Rasterization and ray tracing try to solve the visibility or hidden surface problem but in a different order (the visibility problem was introduced in the lesson <a href="rendering-3d-scene-overview/visibility-problem.html">Rendering an Image of a 3D Scene, an Overview</a>). Both algorithms have in common that they essentially use techniques from geometry to solve that problem. In this lesson, we will describe briefly how the rasterization (you can write rasterisation if you prefer UK English to US English) algorithm works. Understanding the principle is quite simple but implementing it requires to use a series of techniques notably from the field of geometry, that you will also find explained in this lesson.</p>

<p>The program we will develop in this lesson to demonstrate how rasterization works in practice is important, because we will use it again in the next lessons to implement the ray-tracing algorithm as well. Having both algorithms implemented in the same program will allow us to more easily compare the output produced by the two rendering techniques (they should both produce the same result at least before shading is applied) and performances. It will be great way of better understanding the pros and cons of both algorithms.</p>

<h2>The Rasterization Algorithm</h2>

<p>In fact there is not one but multiple rasterization algorithms, but to go straight to the point, let's say that all these different algorithms though are based upon the same overall principle. In other words, all these algorithms are just variants of the same idea. It is this idea or principle, we will refer to when we actually speak of rasterization in this lesson.</p>

<p>What is that idea? In the previous lessons, we already talked about the difference between rasterization and ray-tracing. We also suggested that the rendering process can essentially be decomposed into two main tasks: visibility and shading. Rasterization to say things quickly, is essentially a method to solve the <b>visibility problem</b>. Visibility consists of being able to tell which parts of 3D objects are visible to the camera. Some parts of these objects can be bidden because they are either outside the camera's visible area or hidden by others objects.</p>

<div style="width: 400;" class="captionImage right">
<img class="left"  src="../../images/upload/rasterization/raytracing-rasterd41d.png?"/><p class="caption left">Figure 1: in ray tracing, we trace a ray passing through the center of each pixel in the image and then test if this ray intersect any geometry in the scene. If it an intersection is found, we set the pixel color with the color of the object the ray intersected. Because a ray may intersect several objects, we need to keep track of closest intersection distance.</p>
</div>

<p>Solving this problem can be done in essentially two ways. You can either trace a ray through every pixel in the image to find out the distance between the camera and any object this ray intersects (if any). The object visible through that pixel is the object with the smallest intersection distance (generally denoted t). This is the technique used in ray tracing. Note that in this particular case, you create an image by looping over all pixels in the image, tracing a ray for each one of these pixels, and then finding out if these rays intersect any of the objects in the scene. In other words, the algorithm requires two main loops. The outer loop, iterates over the pixel in the image and the inner loop iterates over the objects in the scene:</p>

<div name="code" class="code">for (each pixel in image) {
    Ray R = computeRayPassingThroughPixel(x,y);
    float tclosest = INFINITY;
    Triangle triangleClosest = NULL;
    for (each triangle in scene) {
        float thit;
        if (intersect(R, object, thit)) {
             if (thit < closest) {
                 triangleClosest = triangle;
             }
        }
    }
    if (triangleClosest) {
        imageAtPixel(x,y) = triangleColorAtHitPoint(triangle, tclosest);
    }
}</div>

<p>Note that in this example, the objects are actually considered to be made of triangles (and triangles only). Rather than iterating other the objects, we just consider the objects as a pool of triangles and iterate other the triangles instead. For reasons we have already explained in the previous lessons, the triangle is often used as the basic rendering primitive both in ray tracing and in rasterization (GPUs requires the geometry to be triangulated).</p>

<p>Ray tracing is the first possible approach to solve the visibility problem. We say the technique is <b>image centric</b> because we shoot rays from the camera into the scene (we start from the image) as opposed to the other way around, which is the approach we will be using in rasterization.</p>

<div style="width: 400;" class="captionImage right">
<img class="left"  src="../../images/upload/rasterization/raytracing-raster2d41d.png?"/><p class="caption left">Figure 2: rasterization can be roughly decomposed in two steps. We first project the 3D vertices making up triangles onto the screen using perspective projection. Then, we loop over all pixel in the image and test whether they lie within the resulting 2D triangles. If they do, we fill the pixel with the triangle's color.</p>
</div>

<p>Rasterization takes the opposite approach. To solve for visibility, it actually "projects" triangles onto the screen, in other words, we go from a 3D representation to a 2D representation of that triangle, using perspective projection. This can easily be done by projecting the vertices making up the triangle onto the screen (using perspective projection as we just explained). The next step in the algorithm is to use some technique to fill up all the pixels of the image that are covered by that 2D triangle. These two steps are illustrated in figure 2. From a technical point of view, they are very simple to perform. The projection steps only requires a perspective divide and a remapping of the resulting coordinates from image space to raster space, a process we already covered in the previous lessons. Finding out which pixels in the image the resulting triangles cover, is also very simple and will be described later.</p>

<p>What does the algorithm look like compared to the ray tracing approach? First, note that rather than iterating over all the pixels in the image first, in rasterization, in the outer loop, we need to iterate over all the triangles in the scene. Then, in the inner loop, we iterate over all pixels in the image, and find out if the current pixel is "contained" within the "projected image" of the current triangle (figure 2). In other words, the inner and outer loops of the two algorithms are swapped.</p> 

<div name="code" class="code">// rasterization algorithm
for (each triangle in scene) {
    // STEP 1: project vertices of the triangle using perspective projection
    Vec2f v0 = perspectiveProject(triangle[i].v0);
    Vec2f v1 = perspectiveProject(triangle[i].v1);
    Vec2f v2 = perspectiveProject(triangle[i].v2);
    for (each pixel in image) {
        // STEP 2: is this pixel contained in the projected image of the triangle?
        if (pixelContainedIn2DTriangle(v0, v1, v2, x, y)) {
            image(x,y) = triangle[i].color;
        }
    }
}</div>

<p>This algorithm is <b>object centric</b> because we actually start from the geometry and walk our way back to the image as opposed to the approach used in ray tracing where we started from the image and walked our way back into the scene.</p>

<p>Both algorithms are simple in their principle, but they differ slightly in their complexity when it comes to actually implementing them and finding solutions to the different problems they require to solve. In ray tracing, actually generating the rays is simple but finding the intersection of the ray with the geometry can reveal itself to be difficult (depending on the type of geometry you deal with) and is also potentially computationally expensive. But let's ignore ray tracing for now. In the rasterization algorithm we need to project vertices onto the screen which is simple and fast, and we will see that the second step which requires to actually find out if a pixel is contained within the 2D representation of a triangle has an equally simple geometric solution. In other words, computing an image using the rasterization approach relies on two very simple and fast techniques (the perspective process and finding out if a pixel lies within a 2D triangle). Rasterization is a good example of an "elegant" algorithm. The techniques it relies on have simple solutions; they are also easy to implement and produce predictable results. For all these reasons, the algorithm is very well suited for the GPU and is actually the rendering technique applied by GPUs to generate images of 3D objects (it can also easily be run in parallel).</p>

<p>In summary:</p>

<ul>
<li>Converting geometry to triangles makes the process simpler. If all primitives are converted to the triangle primitive, we can write fast and efficient functions to project triangles onto the screen and check if pixels lie within these 2D triangles</li>
<li>Rasterization is <b>object centric</b>. We project geometry onto the screen and determine their visibility by looping over all pixels in the image.</li>
<li>It relies on mostly two techniques: projecting vertices onto the screen and finding out if a given pixel lies within a 2D triangle.</li>
<li>The rendering pipeline run on GPUs is based on the rasterization algorithm.</li>
</ul>

<div class="scratchquote">The fast rendering of 3D Z-buffered linearly interpolated polygons is a problem that is fundamental to state of the art workstations. In general, the problem consists of two parts: 1) the 3D transformation, projection and light calculation of the vertices, and 2) the rasterization of the polygon into a frame buffer. (A Parallel Algorithm for Polygon Rasterization, Juan Pineda - 1988)</div>

<p>The terms <b>rasterization</b> comes from the fact that polygons (triangles in this case) are decomposed in a way, into pixels and as we know an image made of pixels is called a raster image. Technically this process is referred to as the <b>rasterization of the triangles into an image of frame buffer</b>.</p>

<div class="scratchquote">Rasterization is the process of determining which pixels are inside a triangle, and nothing more. (Michael Abrash in Rasterization on Larrabee)</div>

<p>Hopefully at this point of the lesson, you have understood the way the image of a 3D scene (made of triangles) is generated using the rasterization approach. Of course what we described so far is the simplest form of the algorithm. First it can be optimized greatly but furthermore we haven't explained yet what happens when two triangles projected onto the screen overlap the same pixels in the image. When that happens, how do we define which one of these two (or more) triangles is visible to the camera. We will now answer these two questions.</p>

<div class="question">What happens if my geometry is not made of triangles? Can I still use the rasterization algorithm? The easiest solution to this problem is to triangulate the geometry. Modern GPUs only render triangles (as well as lines and points) thus you are required to triangulate the geometry anyway. Rendering 3D geometry raises a series of problems which can be more easily resolved with triangles. You will understand why as we progress in the lesson.</div>

<h2>Optimizing: 2D Triangles Bounding Box</h2>

<div style="width: 400;" class="captionImage right">
<img class="left"  src="../../images/upload/rasterization/raytracing-raster3d41d.png?"/><p class="caption left">Figure 3: to avoid iterating over all pixels in the image, we can iterate over all pixels contained in the bounding box of the 2D triangle instead.</p>
</div>

<p>The problem with the naive implementation of the rasterization algorithm we gave so far, is that it requires in the inner loop to iterate over all pixels in the image, even though only a small number of these pixels may be contained within the triangle (as shown in figure 3). Of course this depends on the size of the triangle in the screen. But considering we are not interested in rendering one triangle but an object made up of potentially from a few hundreds to a few millions triangles, it is unlikely that in a typical production example, these triangles will be very large in the image.</p>

<div style="width: 400;" class="captionImage right">
<img class="left"  src="../../images/upload/rasterization/raytracing-raster4d41d.png?"/><p class="caption left">Figure 4: once the bounding box around the triangle is computed, we can loop over all pixels contained in the bounding box and test if they overlap the 2D triangle.</p>
</div>

<p>There are different ways of minimising the number of tested pixels, but the most common one consists of computing the 2D bounding box of the projected triangle, and iterating over the pixels contained in that 2D bounding box rather than the pixels of the entire image. While some of these pixels might still lie outside the triangle, at least on  average, it can already considerably improve the performance of the algorithm. This idea is illustrated in figure 3.</p>

<p>Computing the 2D bounding box of a triangle is actually very simple. We just need to find the minimum and maximum x- and y-coordinates of the three vertices making up the triangle in raster space. This is illustrated in the following pseudo code:</p>

<div name="code" class="code">// convert the vertices of the current triangle to raster space
Vec2f bbmin = INFINITY, bbmax = -INFINITY;
Vec2f vproj[3];
for (int i = 0; i < 3; ++i) {
    vproj[i] = projectAndConvertToNDC(triangle[i].v[i]);
    // coordinates are in raster space but still floats not integers
    vproj[i].x *= imageWidth;
    vproj[i].y *= imageHeight;
    if (vproj[i].x < bbmin.x) bbmin.x = vproj[i].x);
    if (vproj[i].y < bbmin.y) bbmin.y = vproj[i].y);
    if (vproj[i].x > bbmax.x) bbmax.x = vproj[i].x);
    if (vproj[i].y > bbmax.y) bbmax.y = vproj[i].y);
}
</div>

<p>Once we calculated the 2D bounding box of the triangle (in raster space), we just need to loop over the pixel defined by that box. But you need to be very careful about the way you convert the raster coordinates, which in our code are defined as floats rather than integers. First note that one or two vertices may be projected outside the boundaries of the canvas. Thus, their raster coordinates may be lower than 0 or greater than the image size. We solve this problem by clamping the pixel coordinates to the range [0, Image Width - 1] for the x coordinate, and [0, Image Height - 1] for the y coordinate. Furthermore we will need to round off the minimum and maximum coordinates of the bounding box to the nearest integer value (note that this works fine when we iterate over the pixels in the loop, because we initialize the variable to xmim or ymin and break from the loop when the variable x or y is lower or equal to xmas or ymax). All these tests needs to be applied before using the final fixed point (or integer) bounding box coordinates in the loop. Here is the pseudo-code:</p>

<div name="code" class="code">...
uint xmin = std::max(0, std:min(imageWidth - 1, std::floor(min.x)));
uint ymin = std::max(0, std:min(imageHeight - 1, std::floor(min.y)));
uint xmax = std::max(0, std:min(imageWidth - 1, std::floor(max.x)));
uint ymax = std::max(0, std:min(imageHeight - 1, std::floor(max.y)));
for (y = ymin; y <= ymin; ++y) {
    for (x = xmin; x <= xmax; ++x) {
        // check of if current pixel lies in triangle
        if (pixelContainedIn2DTriangle(v0, v1, v2, x, y)) {
            image(x,y) = triangle[i].color;
        }
    }
}</div>

<div class="question">Note that in production rasterizer, there are more efficient methods than looping over the pixels contained in the bounding box of the triangle. As mentioned, many of the pixels do not overlap the triangle and testing if these pixel samples overlap the triangle is a waste. We won't study these more optimized methods in this lesson.</div> 

<div class="question">If you already studied this algorithm or studied how GPUs render images, you may have heard or read that the coordinates of projected vertices are sometimes being converted from floating point to <b>fixed point numbers</b> (in other words integers). The reason behind this conversion is because basic operation such as multiplication, division, addition, etc. on fixed point numbers can be done very quickly (compared to the time it takes to do the same operations with floating point numbers). This used to be the case in the past and GPUs are still designed to work with integers at the rasterization stage of the rendering pipeline. However modern CPUs generally have FPUs (floating-point units) so if your program run on the CPU, there is probably little to no advantage to use fixed point numbers (it actually might even run slower).</div>


<h2>The Image or Frame-Buffer</h2>

<p>Our goal is to produce an image of the scene. We have two ways of visualizing the result of the program, either by displaying the rendered image directly to the screen or saving the image to disk, and using a program such as Photoshop to preview the image later on. But in both cases though, we somehow need to store the image that is being rendered while it's being rendered and for that purpose, we use what we call in CG an image or <b>frame-buffer</b>. It is nothing else than a two-dimensional array of colors that has the size of the image. Before the rendering process starts, the frame-buffer is created and the pixels are all set to black. At render time, when the triangles are rasterized, if a given pixel overlaps a given triangle, then we store the color of that triangle in the frame-buffer at that pixel location. When all triangles have been rasterized, the frame-buffer will contain the image of the scene. All that is left to do then is either displaying the content of the buffer to the screen, or save its content to a file. In this lesson, we will choose the latter option.</p>

<div class="question">In programming, there is no solution to display images to the screen that is cross-platform (which is actually a shame). For this reason, it is better to store the content of the image to a file and use a cross platform application such as Photoshop or another image editing tool to view the image. Of course, the software you will be using to view the image needs to support the image format the image will be saved in. In this lesson we will use the very simple PPM image file format.</div>

<h2>When Two Triangles Overlap the Same Pixel: The Depth Buffer (or Z-Buffer)</h2>

<p>Keep in mind that the goal of the rasterization algorithm is to solve the visibility problem. In order to display 3D objects, it is necessary to determine which surfaces are visible. In the early days of computer graphics two method were used to solve the "hidden surface" problem (the other name for the visibility problem): the <b>Newell algorithm</b> and the <b>z-buffer</b>. We only mention the Newell algorithm for historical reasons but we won't study it in this lesson because it is actually not used anymore. We will only study the z-buffer method which is used by GPUs.</p>

<div style="width: 400;" class="captionImage right">
<img class="left"  src="../../images/upload/rasterization/raytracing-raster5d41d.png?"/><p class="caption left">Figure 5: when a pixel overlaps two triangles, we set the pixel color to the color of the triangle with the smallest distance to camera.</p>
</div>

<p>There is one last thing though that we need to do in order to get a basic rasterizer working. We need to account for the fact that more than one triangle may overlap the same pixel in the image (as shown in figure 5). When this happens, how do we decide which triangle is visible? The solution to this problem is actually very simple. We will use what we call a <b>z-buffer</b> which is also called a <b>depth buffer</b>, two terms that you may have heard or read about already quite often. A z-buffer is nothing more than another two-dimensional array which has the same dimension than the image, however rather than being an array of colors, it is simply an array of floating numbers. Before we start rendering the image, we initialise each pixel in this array to a very large number. When a pixel overlaps a triangle, we also read the value stored in the z-buffer at that pixel location. As you maybe guessed, this array is used to store the distance from the camera to the nearest triangle that any pixel in the image overlaps. Since this value is initially set to infinity (or any very large number), then of course, the first time we find that a given pixel X overlaps a triangle T1, the distance from the camera to that triangle is necessarily lower than the value stored in the z-buffer. What we do then, is replace the value stored for that pixel with the distance to T1. Next, when the same pixel X is tested and that we find that it overlaps another triangle T2, we then compare the distance of the camera to this new triangle to the distance stored in the z-buffer (which at this point, stores to the distance to the first triangle T1). If this distance to the second triangle is lower than the distance to the first triangle, then T2 is visible and T1 is hidden by T2. Otherwise T1 is hidden by T2, and T2 is visible. In the first case, we update the value in the z-buffer with the distance to T2 and in the second case, the z-buffer doesn't need to be updated since the first triangle T1 is still the closest triangle we found for that pixel so far. As you can see <b>the z-buffer is used to store the distance of each pixel to the nearest object in the scence</b> (we don't really use the distance, but we will give the details further in the lesson). In figure 5, we can see that the red triangle is behind the green triangle in 3D space. If we were to render the red triangle first, and the green triangle second, for a pixel that would overlap both triangles, we would have to store in the z-buffer at that pixel location, first a very large number (that happens when the z-buffer is initialized), then the distance to red triangle and then finally the distance to the green triangle.</p>

<p>You may wonder how we find the distance from the camera to the triangle. Let's first look at an implementation of this algorithm in pseudo-code and we will come back to this point later (for now let's just assume the function <span class="code-inline">pixelContainedIn2DTriangle</span> computes that distance for us):</p>

<div name="code" class="code">// A z-buffer is just an 2D array of floats
float buffer = new float [imageWidth * imageHeight];
// initialize the distance for each pixel to a very large number
for (uint32_t i = 0; i < imageWidth * imageHeight; ++i)
    buffer[i] = INFINITY;

for (each triangle in scene) {
    // project vertices
    ...
    // compute bbox of the projected triangle
    ...
    for (y = ymin; y <= ymin; ++y) {
        for (x = xmin; x <= xmax; ++x) {
            // check of if current pixel lies in triangle
            float z; // distance from the camera to the triangle
            if (pixelContainedIn2DTriangle(v0, v1, v2, x, y, z)) {
                // If the distance to that triangle is lower than the distance stored in the
                // z-buffer, update the z-buffer and update the image at pixel location (x,y)
                // with the color of that triangle
                if (z < zbuffer(x,y)) {
                    zbuffer(x,y) = z;
                    image(x,y) = triangle[i].color;
                }
            }
        }
    }
}</div>

<h2>What's Next?</h2>

<p>Obviously this is only a very high-level description of the algorithm (figure 6) but this should hopefully already give you an idea of what we will need in the program in order to produce an image. We will need:</p>
<ul>
<li>An image-buffer (a 2D array of colors),</li>
<li>A depth-buffer (a 2D array of floats),</li>
<li>Triangles (the geometry making up the scene),</li>
<li>A function to project vertices of the triangles onto the canvas,</li>
<li>A function to rasterize the projected triangles,</li>
<li>Some code to save the content of the image buffer to disk.</li>
</ul>

<div style="width: 450;" class="captionImage right">
<img class="left"  src="../../images/upload/rasterization/rasterization-schemad41d.png?"/><p class="caption left">Figure 6: schematic view of the rasterization algorithm.</p>
</div>

<p>In the next chapter we will see how are coordinates converted from camera to raster space. The method is of course identical to the one we studied and presented in the previous lesson, however we will present a few more tricks along the way. In chapter three, we will actually learn how to rasterize triangles. In chapter four, we will study in details how the z-buffer algorithm works. As usual, we will conclude this lesson with a practical example.</p></article></div><!-- end of page-content -->
			<div class='footer-prev-next'>
			<div class='footer-column'><div class='footer-prev-next-row'>
				<div class='footer-prev-next-cell' style='text-align: left;'></div>
				<div class='footer-prev-next-cell' style='text-align: center;'><p>Chapter 1 of  7</p></div>
				<div class='footer-prev-next-cell' style='text-align: right;'><p><a href='rasterization-practical-implementation/projection-stage.html'>Next Chapter <i style='float: right;' class='material-icons'>arrow_forward</i></a></p></div>
			</div></div>
			</div>

</body>

<!-- Mirrored from www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 01 Aug 2021 16:45:05 GMT -->
</html>