Statistics for Data Science - 1
Professor Usha Mohan
Department of Management Studies
Indian Institute of Technology Madras
Lecture 9.5
Variance of a Random Variable
(Refer Slide Time: 00:28)
Then next thing which we saw is let us see a few properties of the variance of a random variable.
Recall when we wanted to look at properties of expectation, we want we saw what is
what was
variable that is what would happen to the expectation if I multiply my
random variable with a constant and if I add a constant to my random variable if you recall these
were the properties when I looked at the expected value of a random variable. Now, let us
continue with the same exercise for variance of a random variable.
(Refer Slide Time: 1:08)
So, the question is, suppose I have X is a random variable and I am multiplying X with the
constant I want to know what this variance of X. Recall
principles I know
. So, from my first
, which is nothing but
which is going to be
which is going to
(Refer Slide Time: 2:01)
So, we can choose and we can see that if X is a random variable and c is a constant then
. Now, what would happen to
, again recall
. So,
these two cancel out which is
. So, this is going to be
(Refer Slide Time: 2:59)
So, variance of a constant times a random variable is c square times variance of X, whereas
variance of X plus a constant is the same as variance of X. Again why is this? Remember if I add
a constant two variables, so if this is the variability and I add a constant the variance does not
change. This is something which if I am adding 1 then the variance does not change the means
changes but the variance does not change if I add a constant because it just going to be a shift in
my distribution. So, I can generalize this result as a corollary, which was very similar to
I get
The proof this is also very simple,
, which is the
expectation of aX plus b and minus b cancel out. So, I get
, and that is what I have here. Hence, I know that if a and b are
constants X
(Refer Slide Time: 04:52)
. These are very important properties of variance.
And let us apply this to understand how to get the variance of sum of random variables. Again
recall when I had random variables X and Y expectation of X plus Y was the sum of expectation
that is expectation of sum was sum of expectations
. So, the question
we are asking is variance of sum will it be equal to the sum of the variances that is the question
we are asking. Now, let us look at the following case. Let X be equal to X and Y be also equal to
X then I know
. Let me tell you I am going to write Var(X) and V(X) they
mean the same this is just a short hand notation to represent variance of X.
So, V(X )and Va (X) and V(Y) and Var(Y) represent the same quantity, which is variance of a
random variable. So,
hence
. We already know
. Now, if I wanted to know whether
, so if I am going to look at the right hand side I get
, I can see
So, in general we ask whether
the answer is no. It need not be
equal and we illustrated it with an example showing that variance of X plus X is not equal to
variance of X plus variance of X. So, the natural question is this always true, is this always true
that is given two random variables variance of X plus Y is
Again the answer is no.
(Refer Slide Time: 07:40)
So, we introduced the notion of independent random variables. What do we mean by random
variables that are independent? I say that two random variables X and Y are independent if
knowing the value of one of them does not change the probability of the other. Let us, look at an
example. Again roll a dice twice, I know this is my sample space.
Now, let X be a random variable, which is the outcome of the first dice and let Y be the outcome
of the second dice I know X takes the value 1, 2, 3, 4, 5, 6, Y also takes the value 1, 2, 3, 4, 5, 6.
Now, X the P(X+1) is independent of what was the outcome of X. So, X and Y are independent
because I, given that X has taken some that is given the outcome of the first dice does not impact
the outcome of the second where Y is defined as the outcome of the second dice, hence X and Y
are independent random variables. Now, why does this become important to us?
(Refer Slide Time: 09:24)
Now, if I have X and Y as independent random variables, then I can show that
. In other words, variance of X plus Y need not always be equal to the sum of
variances. However, if X and Y are independent random variables then the variance of the sum is
the sum of the variances.
(Refer Slide Time: 10:03)
Now, we are going to look at application of this result. Again, let us look at rolling a dice twice
again X is the outcome of the first dice and Y is the outcome of the second toss or first fair dice
and second fair dice. Now, I now that
and we also verified
when I am rolling a dice twice you recall that the expectation of the sum of
dice was equal to 7.
We already computed the variance of X in a roll of a single dice. Now, you recall again X is a
uniform distribution,
verify that
hence
and you can
, which is almost equal to 5.83 which you can
check is 70/12. So, you can verify that this is what we got applying the computational formula,
this is what we got by applying the fact that variance of X plus Y equal to variance of X plus
variance of Y when X and Y are independent random variables.
(Refer Slide Time: 12:03)
Now, let us look at the case of a hyper geometric random variable. We already introduced the
probability mass function. We also check that if I have a hyper geometric variable with
parameters n, m and N we already established
we can verify that
] given by this quantity the proof of this would be discussed in a
tutorial.
(Refer Slide Time: 12:38)
Now, I can extend this property that
to many independent
variable in particular, if I have X1, X2 Xk which are k independent random variables,
extend this not only to two but to k variable. So, variance of sum is sum of variances.
(Refer Slide Time: 13:26)
. I can
Now, how do I apply this formula? Again recall tossing a coin three times is same as noting
down the outcomes of a fair toss three times that is I am repeating an experiment three times. So,
I have a first toss, I have a second toss, I have a third toss. Let X1 be my outcome of the first toss,
X2 be the outcome of the second toss, X3 is outcome of the third toss I know X1 takes the value 0
or 1 which represents tail or head, X2 represents tail or head against 0 or 1, X3 tail or head 0 or 1
since it is a fair coin it takes the value 1/2, 1/2, 1/2, 1/2 and 1/2, 1/2, these are the probabilities
with Xi take value i.
We also no expectation of
are the outcomes of
my first toss, second toss, and third toss I know they are independent of each other
and this is precisely the expectation of a random variable where I am
counting the number of heads we have checked this also which is going to be 3/2.
Now, let us look at the
. If X1 takes the value 0 and 1 I know X1 square also takes the
value 0 and 1 with probability 0 and 1/2 recall this is a Bernoulli random variable. We know the
variance of a Bernoulli random variable is
. Similarly,
in this case is 1/2. So,
variables with parameter
. Because X1, X2, X3 are all Bernoulli random
They are independent also so I can apply my property that the variance of a sum is sum of
variances which we have just seen, so,
, and you can see that matches with the variance which we computed
using the computation formula which is 0.75 and 3/4 is the same this is what we have already
seen.
(Refer Slide Time: 16:50)
So, in summary what we have seen so far is the main properties of the variance namely
which
, where
generalized
are independent random variables. So, these
are the two important properties we have seen and we computed the earlier distributions applying
this property.
