Statistics for Data Science 
Professor Usha Mohan
Department of Management Studies
Indian Institute of Technology, Madras
Lecture 9.4
Variance of a Random Variable
(Refer Slide Time: 00:18)
So what we have seen so far is the following we have looked at what was a random variable,
we defined what is a random variable and then we looked at the types of random variable
namely we said that there are two main types of a random variable we can classify them as
discrete random variable and continuous random variables.
Then we looked at when we focus our, we restricted our focus only to discrete random
variables, in discrete random variables we defined what was a probability mass function in the
sense that you are interested in knowing about a distribution of a random variable, hence we
introduced the notion of a probability mass function.
The probability mass function tells us about the probability distribution of a random variable
and the cumulative distribution function helps us to know what is the overall distribution of the
random variable given a real line. So, we also looked at the graphs of both the probability mass
function and the cumulative distribution function.
Then we went out to introduce an extremely important notion of a random variable namely the
expectation of a random variable, we focused on properties of a random variable and then after
this we looked at how we you can compute the expectation of functions of a random variable.
Today we are going to focus on another important summary of a random variable which we
refer to as a variance of a random variable.
(Refer Slide Time: 01:54)
So, let us first of all motivate the need for such a measure. So we saw that we also realize that
we can interpret the expected value as a long-run average, so in other words the way we
introduced the expected value of a random variable was as a weighted average of possible
values the random variable can take, in other words if X can take values 
! , 
" , 
# . I am
assuming finitely many with 
$ ) equal to 
! , 
" , 
# . These are I assume 
" , 
# are the weights given to the values, then I know that the expectation of X can be
written as 
! + 
" + 
# . In other words, it is 
 is taking possible values
! , 
" , 
# with weights 
! , 
" , 
# , then expectation of 
 can be expressed as a weighted
average of the possible values of random variable.
(Refer Slide Time: 3:11)
Now, let us look at an example where I have three random variables. So, what are these three
random variables, 
 is a random variable with probability 1, 
 is a random variable which takes
the values 
2 with probability 1/2 and +2 with probability 1/2 and 
 is a random variable
which takes the value 
20 with probability 1/2 and +20 with probability 1/2. Now if you
compute 
, I can write 
 takes the value 0 with 
) = 1 which gives me the 
 1 = 0.
Similarly, I know 
 takes the value 
2 and +2 with 
) = 1/2 and 1/2 giving me
) = 
 " + 2 
 1/2 = 0. 
 takes the value 
20 and 20 with 
) is again 1/2 and
1/2 which gives me 
), which is equal to = 
 " + 20 
 1/2 which is again 0.
So, I can see that 
) = 
), so this expected value however what we notice is
though the expectation is same the distribution of these random variables with the terms of the
values they take is different. In other words, the expected value of a random variable does not
tell us anything about the spread or the variation. Hence, I need a measure for the spread.
(Refer Slide Time: 5:10)
What I can notice here although is the spread of 
 is greater than the spread of 
 which is
greater than the spread of 
. The expected value although was the same 
) which is equal to 0. Hence, I need a measure of spread and just as what we did in
a descriptive statistics module mean gave us a measure of central tendency variance gave us a
measure of spread.
(Refer Slide Time: 05:45)
So, we are going to define the variance of a random variable, which is a measure of spread. So,
what how do I define the variance of a random variable? So, let me denote the expected value
of a random variable by the Greek alphabet 
 that is 
) = 
 is my random variable I see
how far is 
 from 
, I take the square of the difference and the expected value of the squared
difference and that is what I am going to define as my variance of 
. So, the if 
 is a random
variable with expected value 
 then the 
) is defined by 
)" .
(Refer Slide Time: 6:43)
In other words, the variance of a random variable defines the square of the difference of the
random variable from its mean on an average. I repeat, the variance of a random variable
measures the difference of the random variable from its mean, so that is (
), it measures
the square of the difference which is (
)" on an average is 
)" .
(Refer Slide Time: 07:18)
So, when I talk about 
)" the question is can I computationally have a more convenient
formula to compute the variance of a random variable? I know variance of a random variable
)" . Now, let us look at (
)" , I know (
)" is 
 " + 
. Now, recall
from my expectation or properties of expectation, 
) where 
 and 
 are constants as
) + 
So, 
 is a constant here, so if I am looking at 
)" , I know this is equal to 
 " ) + 
) which I can write it as 
 " ) + 
" ) is going to be 
), I know 
, hence this reduces to 
 " ) + 
 which gives me 
 " ) 
" (Refer Slide Time:
08:54)
So, the computational formula for 
 is a very simple formula which is the same as 
)" , where 
 is 
). So, now let us apply this formula to compute the variance of the
random variables which we have discussed earlier.
(Refer Slide Time: 09:16)
Let us begin with the role of a fair dice and I am rolling it once I know the sample space is
{1, 2, 3, 4, 5, 6}, 
 is the outcome of the roll, so my 
 takes values 1, 2, 3, 4, 5, 6, I am going
to apply the formula the computational formula 
) = 
 " ) 
)B or 
 " ) 
That is the formula I am going to apply. So, if 
 takes the value 1, 2, 3, 4, 5, 6, I know 
 " takes
the value 1" , 2" is a 4, 3" is a 9, 4" is a 16, 5" is a 25 and 6" is a 36.
Now, if 
 takes the value 1, 
 " takes the value 1 with the same probability. Similarly, if 
 takes
the value 2, 
 " takes the value 4 with probability 1/6, 
 takes the value 3, 
 " takes the value
9 with probability 1/6 and so forth 
 takes the value 
 " takes the value 
 " with probability
1/6.
(Refer Slide Time: 10:45)
We have already computed expectation of 
 and we notice that 
) was 3.5. So, what is
 " )? 
 " takes the value 1 with probability 1/6, 4 with probability 1/6, 9 with probability
1/6, 16 with probability 1/6, 25 with probability 1/6and 36 with probability 1/6, which gives
 " ) is 15.167, if 
) is 3.5 
)" is 3.5" and hence my 
) is 15.167 which is
 " ) 
" which is 2.917.
(Refer Slide Time: 11:41)
Similarly, let us look at the example of rolling a dice twice, again we know that 
 takes values
2, 3, 4 up to 12, hence 
 " takes the values 4, 9, 16, 25 up to 144, again the probability with
which 
 takes the value 2 is same as the probability with which 
 takes the value 
 " takes the
value 4 which is equal to 1/6 in this case and hence my 
) we have already computed was
7, I can verify that 
 " ) is 54.833 giving me 
) is 5.833.
(Refer Slide Time: 12:26)
So, we notice the following that in the earlier case when 
 was a roll of a dice it took the value
1, 2, 3, 4, 5, 6, it had equal probability and now you can see that the mean was around 3.5 this
was somehow balancing it, here the mean is at 7 and I can see that the variance in this case is
5.833 because it is taking a value from 2 to 12, so the variability is more, here it is taking the
value 1 to 6 and it had a variance of about 2.91.
(Refer Slide Time: 13:09)
Now, let us apply the formula to tossing a coin thrice, again I know that 
 is a random variable
which takes the values, again I know 
 is a random variable which takes the values 0, 1, 2, 3,
hence 
 " is equal to 0, 1, 4 and 9 with the same probabilities, so I can verify that 
), I
already have checked.
(Refer Slide Time: 13:37)
!))(!
*))(+
*))(,
) was 3/2, 
 " ) is going to be
can verify that to be 24/8, which is 3.
that would be my 
 " ) which I
(Refer Slide Time: 14:05)
Hence, my 
) is 3 
 2.25 which is 0.75, because this is 1.5, 1.5" is 2.25, 
) is 3 
2.25 which is a 0.75.
(Refer Slide Time: 14:23)
Again, you can see that the value of 
 is between 0 and 3, so I had when 
 took the value 1, 2,
3, 4, 5, 6 with equal probability I had a variance of 2.91, 
 took the value 2, 3, 4 up to 12 with
varied probabilities, then I had 5.83, now 
 is again taking the value 0, 1, 2, 3 with probability
1/8, 3/8, 3/8 and 1/8, my variance is very less which is 0.75.
(Refer Slide Time: 14:57)
So, now let us look at the specific random variables which we have already defined recall we
defined a Bernoulli random variable as that random variable which takes only 2 values for
convenience I assumed it took the value 0 and 1 with 
) taking value 0 as (1 
) and value
1 equal to 
, if 
 takes the value 0 and 1, 
 " also takes the value 0 and 1 with the same
probability is (1 
) and 
We verified that 
) is 0 
 (1 
) + 1 
, which gave me the 
) equal to 
 " ) is
also 0 
 (1 
) + 1 
 which is 
, hence the variance equal to 
 " ) which is equal to 
)" which is 
" , which I can write as 
). Hence the variance of a Bernoulli random
variable is 
). So, I can refer to this as if 
 is a Bernoulli random variable with parameter
, then the E(X) is 
, the 
) is 
(Refer Slide Time: 16:37)
Let us, look at that case of a discrete uniform random variable, again I know I call a random
variable to be a uniformly distributed random variable if 
 takes the value 1, 2, 
 with
probability 1/
, 1/
, 1/
, again an example of this is my roll of a single dice where my 
 was
6 and I knew that 
) equal to each value is 1/6, 1/6, 1/6.
(Refer Slide Time: 17:13)
So, if 
 takes these values then 
 " takes the value 1, 4, 
" , the probabilities remain the
same. We have already seen the 
) = 1 
 # + 2 
 # + 
 # which we showed was
(1 + 2 + 
) which we further saw that this was
(Refer Slide Time: 17:51)
#(#)!)
which gave me
(#)!)
(#)!)
So, I already verified that 
) = " , now when 
 = 6, I know that this is equal to 7/2,
which is 3.5 and this 3.5 actually matches with my expectation of a single roll of a dice that is
3.5, recall, I got a 3.5 when I found out the expectation of rolling a dice 1 was 3.5. Now, from
here remember the variance was 2.91, so let us go and check for a discrete random variable.
So, again when I go and do my check for a discrete random variable I see that what is 
 " )
in this case.
Now, if I look at 
 " , 
 " takes the value 1 + 4 + 
" so I know 
 " ), I can write it as 1
by so 
 " ) I can write it as 1 
 # + 4 
 # + 
 #, so I get # (1 + 2" + 3" + 
" ),
the term within the brackets is nothing but the sum of the squares of the first 
 natural numbers.
(Refer Slide Time: 19:32)
And we can verify and we already know that the 
 " ) is nothing but
sum of squares is
#(#)!)("#)!)
. Now, the 
) is going to be [
(#)!)("#)!)
(#)!)("#)!)
 " ), this is 
)" and I can show that this would be this is going to be a
(#! /!)
, because the
(#)!)!
"#! )*#)!
]. This is
#! )!)"#
which gives me the fact that Var(X)= !" . So, we computed the variance of very well-known
distributions namely the Bernoulli distribution and the discrete uniform distribution.
(Refer Slide Time: 20:57)
We will be looking at a few more distributions later, but for now in summary, we introduced
the notion of a variance, the notion of a variance of a random variable captures the measure of
spread, hence it is an important measure and we also computed through the computational
formula and the computational formula is variance of X is 
 " ) 
)" which is the same
 " ) 
" , we applied this formula to compute the variance of some well-known
distributions and examples that we have already seen.
