Statistics for Data Science - 1
Professor Usha Mohan
Department of Management Studies
Indian Institute of Technology, Madras
Lecture 9.3 - Expectation of a random variable
(Refer Slide Time: 00:15)
Now, suppose I am interested in knowing about, I have a random variable , I have defined
expectation of a random variable
expectation of, say,
. Now, suppose I am interested in knowing about
, or expectation of
, or expectation of
. So, I am expect. In other
words, we are interested in knowing about expectation of a function of .
So, if I have , which is a random variable, which I again takes, for now I am assume it takes
finite number of values, then
is going to take
with values
probabilities would remain the same. So, I have
I know our expectation of
is going to be
I can define expectation of
to be
value
with
, because it takes
, it takes the value
with
with
, it takes the value
. And that is what is summarised here, as expected value of
(Refer Slide Time: 02:34)
As a natural corollary for it, if
Suppose,
. Suppose
know that, again, let
expectation of
can remove
, so suppose I have now
is just a constant times ,
take the value
is going to be
is a constant time , then I
is going to be
outside, I get
recognise this term inside the bracket is
is a constant times .
. So, we can
So, I have, so we can recognise that this value inside this expectation of . Hence, I know I
can write that
is nothing but
case where
, where
where I have both
is a constant. So similarly, let us look at the
are constants. So again, let us look at that case
are constants.
(Refer Slide Time: 04:16)
So, what do I have I have my . So, I have my , which is taking the value
now is going to be
with the same probabilities, this is
, this is going to be
So, my
, my
, this is going to be
is going to be
. Now, we can see that this is nothing but I can write this as
the first portion, and the second I can write it as
Now, the first portion that is this is nothing but
. Now, since I have the second portion,
this is going to be a probability mass function. So, this adds up to 1. Hence, I can check and
verify that
, where both
arises as a general corollary to
(Refer Slide Time: 06:22)
are constant is the same as
is a same as 
. And this
So, now let us look at a simple example where
is a discrete random variable with the
following distribution, again is this a probability mass function
takes the value -1, 0 and 1
yes, this is a probability mass function, because 0.2 + 0.5 + 0.3 equals to 1. So, it is a
probability mass function, I have each probability greater or equal to 0.
Now, let us define
is equal to
. So, let
. So again,
be another random variable, which is equal to
takes the value -1, 0, 1
be 1, 0, 1, with probability 0.2, 0.5 and 0.3, this is
So, my expected value of
takes the value
and 1.
which is going to
the same probability.
, which is my expectation of , is going to be
, which is equal to 0.5. So, I have expectation of
notice
, which
, I can also write this as , which is
0.5. Now, when you
takes 2 values and those values are 0
Now, what is the probability with which
takes the value 0, the probability with which
takes the value 0 is 0.5, and the probability with which
takes the value 1 is 0.2 + 0.3, which
is again a 0.5. Hence, if I looked at from the first definition
, it is going to be
which is equal to 0.5. This matches with what we got earlier.
(Refer Slide Time: 08:39)
Hence, given a random variable, I can find out what is the expectation of the function of a
random variable. A very common mistake people do is to think
, what we are seeing just now is
is -0.2 + 0.3, which is a 0.1. So, the
was 0.5, what is
is 0.01. So,
is the same as
. And this
you should be very careful about understanding this that expectation of square of a random
variable is not the same as square of expectation. So, expectation of square is not same as
square of expectation.
(Refer Slide Time: 09:55)
The next example, where we go for an application of what we have learned is the following
application. So, Sanjay and Anita work for the same company, Anita
s Diwali bonus is a
random variable whose expected value is 15,000. So, let
represents Anita's bonus, what is given to me is
given to us.
be the random variable, which
is 15,000 rupees. So, that is what is
(Refer Slide Time: 10:28)
Now, the next thing that is given to us is Sanjay's bonus is equal to 90% of Anita
s bonus. So,
be Sanjay's bonus
know what is
is Anita
s bonus
this is given to be 15,000 rupees. So, the question is what is expected
value of Sanjay
s bonus that is
is 0.
is Sanjay's bonus what is given to is
. From our earlier result I know that if
which is 0.9 15,000 rupees which is 13,500 rupees.
(Refer Slide Time: 11:32)
is 0.
then
So, I can see that Sanjay
s bonus expected values of Sanjay's bonus is 13,500. But suppose
Sanjay's bonus is set to be equal to 1000 rupees more than that of Anita
s bonus, then find the
expected bonus. So, in this case, I have
already know
. So,
is going to be
+1000. I
is 15,000. I add 1000 hence, the expected value of Sanjay's bonus is
going to be 16,000 rupees.
So, you can see how we have just applied the expectation property of
. In this case, my
was 0.9 and
was 0. In this case
was 0 and
need to identify what are your constants to apply this property.
(Refer Slide Time: 12:51)
was 1000. So, just you
Now, let us look at what can I tell about expectation of a sum of random variables. If I am
given two random variables with and I know that
relative or respective expectations, then
that is expectation of sum is sum of expectations.
(Refer Slide Time: 13:30)
We are not proving these but let us look at example, again roll a dice. If I am rolling a dice
and I say, I roll 1 dice, I roll another dice. The sample space in both these experiments are
going to remain the same because I am just rolling a dice. So, if
here, we have already seen
expectation was which is
Again, a
. So, if
in this case is going to be I have already seen this
is representing the outcome in rolling the second dice, I also know
are outcomes of rolling dice separately, I know
to . Now, what is ,
is denoting the outcome
is the outcome of the first dice,
is again
which is equal
is the outcome of the second die.
would denote the outcome or sum of outcomes of both the dice. Sum of outcomes
of both the dice is equivalent to so rolling a die twice. So, you can see that
my formula is
which is
from
, which is equal to 7. And this matches with our
, which was same as the expectation or sum of outcomes of rolling a dice.
(Refer Slide Time: 15:18)
So, now let us look at a example of another important random variable which arises in a lot of
application. And this is referred to as a hyper geometric random variable, a hyper geometric
random variable. Suppose I have
, I have total of
of 1 type, and I have
balls, I can or
people of which
of another type, I have total
are of one type and
are of
another type.
Here, I am assuming
are red balls, and
are blue balls. From this
, I am choosing a
sample of size . So, size this is my sample of size . Again, this could have, again, this
sample also could have red balls and blue balls, the question we are interested in knowing is
suppose for example, I have 5 balls, of these 5 balls, I have 3 red balls, and I have 2 blue
balls.
And I am choosing from here, 2 balls at random, then the possible choices for these 2 balls is
I could have both are red, I could have 1 red and 1 blue, I could have both are blue balls,
these are the possible chances I can have. So, if I look at the number of red balls in my
sample, the number of red balls here are 2 number of red balls here is 1, the number of red
balls here is 0.
So, in this, if I am choosing both blue balls, I do not have any red ball, if I have 1 blue ball or
and 1 red ball, I have 1 red ball and here I have 2. Similarly, if I am counting the number of
blue balls, here it is a 0, here it is a 1, here it is a 2. So, the random variable of interest in this
case could be the counting the number of red balls, or blue balls, the probability mass
function, for example.
(Refer Slide Time: 17:56)
If I am looking at this example here, so here the probability mass function, where
is the
number of red balls that are chosen. And I have said that this i can take value 0 to . Again,
recall, I have
I am choosing , in this I have
both red balls, or I could have all the
red, I have
blue. So, I could have
red balls, or I could have all the
blue balls. So, I
could have 0 red balls, the probability mass function or the probability distribution is given
by this expression. How we get this expression would be discussed in tutorials, but this is the
expression.
(Refer Slide Time: 18:44)
And you can also verify that this
kind, which takes value 0, 1 to
, and
, a variable of this kind, a discrete variable of this
is referred to as a hyper geometric variable for values of ,
(Refer Slide Time: 19:11)
Let us look at an application of the hyper geometric random variable. I am choosing 2
students from a group of 20 boys and 10 girls. So, my
people, I have 20 boys and 10 girls. So, this was my
is 30. I have 30 people in these 30
, this is my
This is of one type this is of another type. I am choosing
, which is 30
which is equal to 2.
I am choosing 2 students. Again, it could be both girls, both boys or 1 girl and 1 boy. So, my
probability mass function of
20, my
=30 and my
will follow what I have here, ( ) my
here is equal to
, I can plug in these values in this equation of mine, what I am
interested in knowing is the following. So, my
is going to be
, which is
, which is
(Refer Slide Time: 20:38)
So, you can see that the
where I recognise
denoting the number of girls that is chosen. So, if
everything else capital
remains 30, small
equal to 10, because I have only 10 girls.
and =2 is . Now,
is the number of girls that is chosen, now
also remains 2 but my
is now going to be
So, in this case, my
is going to be
which is
, which is
shown to be equal to
again by assuming or by seeing that
with
that is my
can be
is a hyper geometric variable
(Refer Slide Time: 21:44)
Now, what is the
is equal to 2 and I get
, I can check that
, which is
which
(Refer Slide Time: 22:07)
I can extend this property, what is the proper expectation of sum is sum of expectation to
more than 2 variables. In other words, if I have
the expectation of the sum which is equal to
(Refer Slide Time: 22:50)
when
discrete random variables, then
is equal to
it would reduce to what we have discussed earlier.
So, now let us look at the experiment of tossing a coin 3 times, we already know that if
the number of heads, then we have seen that
this is what we have seen already.
Now, let us look at this experiment I find I toss a coin once I toss a coin second or toss 1 coin
toss 2 coin toss third coin let
be the random variable, which takes the value 0 for a tail and
1 for a head, with probability equal likely and . I know
Similarly, let
probabilities
be the random variable again takes the value tail and head with the same
is again a .
again equal to , now
again takes the value 0 and 1 with probability ,
, will is the same as number total number of heads in 3
tosses, or the 3 coins.
Which is same as number of heads, when I toss a coin 3 times, I can verify that
which is
, which is . And this matches with what
we already obtained for the expectation of number of heads, when you toss a coin thrice
which is .
(Refer Slide Time: 25:02)
So, in summary, what we have introduced so far and what you should be knowing now is the
notion of the expected value as a long run average, word of caution is do not interpret it to be
that that is the value the random variable will take. For example, we saw that
cannot take a
value 3.5 when you roll a dice once, but rather it is the average of the outcomes.
The formula to compute an expectation of a random variable, which is 
expectation of a function of a random variable which is 
and expectation of
the sum, which is the same as sum of expectations.
And we looked at how to apply these concepts. So, the next thing which we are going to look
at is what we mean by variance of a random variable. We are going to focus on the variance
of a discrete random variable.
